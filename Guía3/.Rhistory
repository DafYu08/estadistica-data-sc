h <- c(8, 10, 0.001)
vector_1 <- c()
for(j in 1:length(h)){
vector_2 <- c()
for(i in 1:length(muestras)){
f_actual <- f_sombrero(muestras[i], nucleo_gaussiano, muestras[-i], h[j])
vector_2[i] <- log(f_actual)
}
vector_1[j] <- mean(na.omit(vector_2))
}
h_CV <- h[which.max(vector_1)]
return(h_CV)
}
par(mfrow=c(1,1))
# Utilizando la ventana estimada_2 para las densidades 1 y 2
densidad_t1 <- density(est1, kernel = "gauss", window = bw.ucv(est1))
densidad_t2 <- density(est2, kernel = "gauss", window = bw.ucv(est2))
plot(densidad_t2, col = "violet")
lines(densidad_t1, col = "orange")
legend("topright", legend = c("densidad primer estimador varianza", "densidad segundo estimador varianza"), col = c("orange", "violet"), lty = c(1, 1), lwd = c(2, 2))
#Ejercicio e
error1 <- (1/rep) * error1
error1
error2 <- (1/rep) * error2
error2
#Genero datos
set.seed(2023)
rep <- 1000
error_cuadratico_medio <- function(estimador){
return((estimador - 4)**2)
}
muestras <- c()
est1 <- c()
est2 <- c()
error1 <- 0
error2 <- 0
for (i in 1:rep) {
muestra_exp <- rexp(20, 0.5)
muestras <- c(muestras, muestra_exp)
t1 <- mean(muestra_exp)^2
est1 <- c(est1, t1)
t2 <- sd(muestra_exp)^2
est2 <- c(est2, t2)
error1 <- error1 + error_cuadratico_medio(t1)
error2 <- error2 + error_cuadratico_medio(t2)
}
#Ejercicio c
#Armo boxplots paralelos
par(mfrow=c(1,2))
boxplot(est1, col="blue", main = "Primer estimador")
boxplot(est2, col = "green", main = "Segundo estimador")
#Ejercicio d
#Funciones para elegir ventana
nucleo_gaussiano <- function(u){
kernel <- exp(-(u^2)/2)/sqrt(2*pi)
return(kernel)
}
f_sombrero <- function(muestra, kernel, datos, h){
suma <- 0
for(i in 1:length(datos)){
actual <- kernel((muestra-datos[i])/h)
suma <- suma + actual
}
f <- suma / (length(datos) * h)
return(f)
}
'
# Ventana de Silverman (al final no lo usé porque son exponenciales)
h_Silverman <- 1.06 * min(sd(muestras), IQR(muestras) / 1.349) * length(muestras) ^ (-1/5)
# Venta usando Convalidación Cruzada
elegir_ventana_CV <- function(muestras){
h <- c(8, 10, 0.001)
vector_1 <- c()
for(j in 1:length(h)){
vector_2 <- c()
for(i in 1:length(muestras)){
f_actual <- f_sombrero(muestras[i], nucleo_gaussiano, muestras[-i], h[j])
vector_2[i] <- log(f_actual)
}
vector_1[j] <- mean(na.omit(vector_2))
}
h_CV <- h[which.max(vector_1)]
return(h_CV)
}
'
par(mfrow=c(1,1))
# Utilizando la ventana estimada_2 para las densidades 1 y 2
densidad_t1 <- density(est1, kernel = "gauss", window = bw.ucv(est1))
densidad_t2 <- density(est2, kernel = "gauss", window = bw.ucv(est2))
plot(densidad_t2, col = "violet")
lines(densidad_t1, col = "orange")
legend("topright", legend = c("densidad primer estimador varianza", "densidad segundo estimador varianza"), col = c("orange", "violet"), lty = c(1, 1), lwd = c(2, 2))
#Ejercicio e
error1 <- (1/rep) * error1
error1
error2 <- (1/rep) * error2
error2
#Genero datos
set.seed(2023)
rep <- 1000
error_cuadratico_medio <- function(estimador){
return((estimador - 4)**2)
}
muestras <- c()
est1 <- c()
est2 <- c()
error1 <- 0
error2 <- 0
for (i in 1:rep) {
muestra_exp <- rexp(20, 0.5)
muestras <- c(muestras, muestra_exp)
t1 <- mean(muestra_exp)^2
est1 <- c(est1, t1)
t2 <- sd(muestra_exp)^2
est2 <- c(est2, t2)
error1 <- error1 + error_cuadratico_medio(t1)
error2 <- error2 + error_cuadratico_medio(t2)
}
#Ejercicio c
#Armo boxplots paralelos
par(mfrow=c(1,2))
boxplot(est1, col="blue", main = "Primer estimador")
boxplot(est2, col = "green", main = "Segundo estimador")
#Ejercicio d
#Funciones para elegir ventana
nucleo_gaussiano <- function(u){
kernel <- exp(-(u^2)/2)/sqrt(2*pi)
return(kernel)
}
f_sombrero <- function(muestra, kernel, datos, h){
suma <- 0
for(i in 1:length(datos)){
actual <- kernel((muestra-datos[i])/h)
suma <- suma + actual
}
f <- suma / (length(datos) * h)
return(f)
}
'
# Ventana de Silverman (al final no lo usé porque son exponenciales)
h_Silverman <- 1.06 * min(sd(muestras), IQR(muestras) / 1.349) * length(muestras) ^ (-1/5)
# Venta usando Convalidación Cruzada
elegir_ventana_CV <- function(muestras){
h <- c(8, 10, 0.001)
vector_1 <- c()
for(j in 1:length(h)){
vector_2 <- c()
for(i in 1:length(muestras)){
f_actual <- f_sombrero(muestras[i], nucleo_gaussiano, muestras[-i], h[j])
vector_2[i] <- log(f_actual)
}
vector_1[j] <- mean(na.omit(vector_2))
}
h_CV <- h[which.max(vector_1)]
return(h_CV)
}
'
par(mfrow=c(1,1))
# Utilizando la ventana estimada_2 para las densidades 1 y 2
densidad_t1 <- density(est1, kernel = "gauss")# window = bw.ucv(est1))
densidad_t2 <- density(est2, kernel = "gauss", window = bw.ucv(est2))
plot(densidad_t2, col = "violet")
lines(densidad_t1, col = "orange")
legend("topright", legend = c("densidad primer estimador varianza", "densidad segundo estimador varianza"), col = c("orange", "violet"), lty = c(1, 1), lwd = c(2, 2))
#Ejercicio e
error1 <- (1/rep) * error1
error1
error2 <- (1/rep) * error2
error2
library(MASS)
log_verosimilitud_gamma <- function(params, datos) {
alpha <- params[1]
lambda <- params[2]
n <- length(datos)
log_verosimilitud <- n * (alpha * log(lambda) - lgamma(alpha)) +
(alpha - 1) * sum(log(datos)) - lambda * sum(datos)
return(log_verosimilitud)  # Retorno sin negativo para maximize
}
derivada_parcial_alpha <- function(alpha, lambda, datos, n) {
return(n * (digamma(alpha) - log(lambda)) + sum(log(datos)))
}
derivada_parcial_lambda <- function(alpha, lambda, datos, n) {
return(-sum(datos) + n * alpha / lambda)
}
newton_raphson <- function(datos, i) {
n <- length(datos)  # Obtener n
alpha_mom <- mean(datos)^2 / var(datos)
lambda_mom <- mean(datos) / var(datos)
alpha <- alpha_mom
lambda <- lambda_mom
for (iter in 1:i) {
d_alpha_val <- derivada_parcial_alpha(alpha, lambda, datos, n)
d_lambda_val <- derivada_parcial_lambda(alpha, lambda, datos, n)
incremento_alpha <- d_alpha_val / optimize(function(x) log_verosimilitud_gamma(c(x, lambda), datos), interval = c(0.001, 100))$objective
incremento_lambda <- d_lambda_val / optimize(function(x) log_verosimilitud_gamma(c(alpha, x), datos), interval = c(0.001, 100))$objective
alpha <- alpha - incremento_alpha
lambda <- lambda - incremento_lambda
}
return(c(alpha, lambda))
}
# Prueba con diferentes tamaños de muestra y repetir 10 veces
set.seed(456)
tamaños_muestra <- c(6, 10, 20, 40, 80, 200)
shape <- 3
rate <- 4
reps <- 5
# Vectores para almacenar los ECM
ECM_alpha_momentos <- c()
ECM_alpha_MV <- c()
ECM_lambda_momentos <- c()
ECM_lambda_MV <- c()
for (n in tamaños_muestra) {
error_cuadratico_medio_alpha_momentos <- 0
error_cuadratico_medio_alpha_MV <- 0
error_cuadratico_medio_lambda_momentos <- 0
error_cuadratico_medio_lambda_MV <- 0
for (k in 1:reps) {
datos <- rgamma(n, shape = shape, scale = 1/rate)
alpha_momentos <- mean(datos)^2 / var(datos)
lambda_momentos <- mean(datos) / var(datos)
estimador_MV <- newton_raphson(datos, i = 10)
error_cuadratico_medio_alpha_momentos <- error_cuadratico_medio_alpha_momentos + ((alpha_momentos - shape)^2)/reps
error_cuadratico_medio_alpha_MV <- error_cuadratico_medio_alpha_MV + ((rate - estimador_MV[1])^2)/reps
error_cuadratico_medio_lambda_momentos <- error_cuadratico_medio_lambda_momentos + ((shape - lambda_momentos)^2)/reps
error_cuadratico_medio_lambda_MV <- error_cuadratico_medio_lambda_MV + ((rate - estimador_MV[2])^2)/reps
}
# Guardar los ECM en los vectores
ECM_alpha_momentos <- c(ECM_alpha_momentos, error_cuadratico_medio_alpha_momentos)
ECM_alpha_MV <- c(ECM_alpha_MV, error_cuadratico_medio_alpha_MV)
ECM_lambda_momentos <- c(ECM_lambda_momentos, error_cuadratico_medio_lambda_momentos)
ECM_lambda_MV <- c(ECM_lambda_MV, error_cuadratico_medio_lambda_MV)
}
# Graficar los ECM
plot(tamaños_muestra, ECM_alpha_momentos, type = "b", col = "blue", xlab = "Tamaño de muestra", ylab = "ECM", main = "ECM para estimador de momentos (alpha)")
points(tamaños_muestra, ECM_alpha_MV, type = "b", col = "red")
points(tamaños_muestra, ECM_lambda_momentos, type = "b", col = "green")
points(tamaños_muestra, ECM_lambda_MV, type = "b", col = "purple")
legend("topright", legend = c("ECM_alpha_momentos", "ECM_alpha_MV", "ECM_lambda_momentos", "ECM_lambda_MV"), col = c("blue", "red", "green", "purple"), pch = c(1, 1, 1, 1))
?digamma
quad <- function(a, b, c) {
if (a == 0) {
if (b == 0) {
if (c == 0) {
return("Infinitas soluciones")
} else {
return("No tiene solución")
}
} else {
return(-c / b)
}
}
discriminante <- b^2 - 4 * a * c
if (is.complex(discriminante) || is.na(discriminante)) {
return("Raíces complejas")
}
raiz1 <- (-b + sqrt(discriminante)) / (2 * a)
raiz2 <- (-b - sqrt(discriminante)) / (2 * a)
if (identical(raiz1, raiz2)) {
return(raiz1)
} else {
return(c(raiz1, raiz2))
}
}
metodo1 <- function(muestra){
aux <- sqrt((mean(muestra) * (1 - mean(muestra))) / length(muestra))
inferior <- mean(muestra + qnorm(0.025) * aux)
superior <- mean(muestra + qnorm(0.975) * aux)
return(c(inferior, superior))
}
metodo2 <- function(muestra, n){
a = (qnorm(0.025)^2 + n)
b = 2*n*mean(muestra) + qnorm(0.025)^2
c = n * mean(muestra)^2
raices <- quad(a, b, c)
return(raices)
}
k <- 100
n <- 100
p <- 0.4
binomiales <- c()
int_de_confianza_metodo1 <- c()
int_de_confianza_metodo2 <- c()
for (i in 1:k){
binomial <- rbinom(n, 1, p)
binomiales[[i]] <- binomial
longitud1 <- metodo1res(binomial)[2] - metodo1res(binomial)[1]
int_de_confianza_metodo1 <- c(int_de_confianza_metodo1, metodo1(binomial))
int_de_confianza_metodo2 <- c(int_de_confianza_metodo2, metodo2(binomial, n))
}
setwd("~/Estadística/Guía3")
quad <- function(a, b, c) {
if (a == 0) {
if (b == 0) {
if (c == 0) {
return("Infinitas soluciones")
} else {
return("No tiene solución")
}
} else {
return(-c / b)
}
}
discriminante <- b^2 - 4 * a * c
if (is.complex(discriminante) || is.na(discriminante)) {
return("Raíces complejas")
}
raiz1 <- (-b + sqrt(discriminante)) / (2 * a)
raiz2 <- (-b - sqrt(discriminante)) / (2 * a)
if (identical(raiz1, raiz2)) {
return(raiz1)
} else {
return(c(raiz1, raiz2))
}
}
metodo1 <- function(muestra){
aux <- sqrt((mean(muestra) * (1 - mean(muestra))) / length(muestra))
inferior <- mean(muestra + qnorm(0.025) * aux)
superior <- mean(muestra + qnorm(0.975) * aux)
return(c(inferior, superior))
}
metodo2 <- function(muestra, n){
a = (qnorm(0.025)^2 + n)
b = 2*n*mean(muestra) + qnorm(0.025)^2
c = n * mean(muestra)^2
raices <- quad(a, b, c)
return(raices)
}
k <- 100
n <- 100
p <- 0.4
binomiales <- c()
int_de_confianza_metodo1 <- c()
int_de_confianza_metodo2 <- c()
for (i in 1:k){
binomial <- rbinom(n, 1, p)
binomiales[[i]] <- binomial
longitud1 <- metodo1res(binomial)[2] - metodo1res(binomial)[1]
int_de_confianza_metodo1 <- c(int_de_confianza_metodo1, metodo1(binomial))
int_de_confianza_metodo2 <- c(int_de_confianza_metodo2, metodo2(binomial, n))
}
muestra_A <- c(250 ,252 ,245 ,258, 240, 247, 251, 249, 250 ,243 ,247 ,260 ,238 ,241, 239)
mean(muestra_A)
muestra_B <- c(330, 335, 327, 329, 320, 332, 337, 328, 334 ,326 ,331 ,332, 328, 329, 337,
341, 336 ,338 ,325 ,321)
mean(muestra_B)
qnorm(0.005) * sqrt( se(muestra_A) * (1/length(muestra_A) + 1/length(muestra_B)))
qnorm(0.005) * sqrt( sq(muestra_A) * (1/length(muestra_A) + 1/length(muestra_B)))
qnorm(0.005) * sqrt( sd(muestra_A) * (1/length(muestra_A) + 1/length(muestra_B)))
quad <- function(a, b, c) {
if (a == 0) {
if (b == 0) {
if (c == 0) {
return("Infinitas soluciones")
} else {
return("No tiene solución")
}
} else {
return(-c / b)
}
}
discriminante <- b^2 - 4 * a * c
if (is.complex(discriminante) || is.na(discriminante)) {
return("Raíces complejas")
}
raiz1 <- (-b + sqrt(discriminante)) / (2 * a)
raiz2 <- (-b - sqrt(discriminante)) / (2 * a)
if (identical(raiz1, raiz2)) {
return(raiz1)
} else {
return(c(raiz1, raiz2))
}
}
metodo1 <- function(muestra){
aux <- sqrt((mean(muestra) * (1 - mean(muestra))) / length(muestra))
inferior <- mean(muestra + qnorm(0.025) * aux)
superior <- mean(muestra + qnorm(0.975) * aux)
return(c(inferior, superior))
}
metodo2 <- function(muestra, n){
a = (qnorm(0.025)^2 + n)
b = 2*n*mean(muestra) + qnorm(0.025)^2
c = n * mean(muestra)^2
raices <- quad(a, b, c)
return(raices)
}
k <- 100
n <- 100
p <- 0.4
binomiales <- c()
int_de_confianza_metodo1 <- c()
int_de_confianza_metodo2 <- c()
for (i in 1:k){
binomial <- rbinom(n, 1, p)
binomiales[[i]] <- binomial
longitud1 <- metodo1res(binomial)[2] - metodo1res(binomial)[1]
int_de_confianza_metodo1 <- c(int_de_confianza_metodo1, metodo1(binomial))
int_de_confianza_metodo2 <- c(int_de_confianza_metodo2, metodo2(binomial, n))
}
datos <- rgamma(100, 2, 0.5)
?trigamma
log_verosimilitud_gamma <- function(params, datos) {
alpha <- params[1]
lambda <- params[2]
n <- length(datos)
log_verosimilitud <- n * (alpha * log(lambda) - lgamma(alpha)) +
(alpha - 1) * sum(log(datos)) - lambda * sum(datos)
return(-log_verosimilitud)  # Se retorna el negativo para maximizar
}
derivada_parcial_alpha <- function(alpha, lambda, datos) {
n <- length(datos)
return(n * (digamma(alpha) - log(lambda)) + sum(log(datos)))
}
derivada_parcial_lambda <- function(alpha, lambda, datos) {
return(-sum(datos) + n * alpha / lambda)
}
newton_raphson <- function(datos, i) {
#Arranco con los de momentos como X_0
alpha_mom <- mean(datos)^2 / var(datos)
lambda_mom <- mean(datos) / var(datos)
alpha <- alpha_mom
lambda <- lambda_mom
for (iter in 1:i) {
# Calcular derivadas
d_alpha_val <- derivada_parcial_alpha(alpha, lambda, datos)
d_lambda_val <- derivada_parcial_lambda(alpha, lambda, datos)
# Calcular incremento de parámetros. Lo de optimize me lo sugirió chat-gpt para que queden mejor las divisiones
incremento_alpha <- d_alpha_val / optimize(function(x) -log_verosimilitud_gamma(c(x, lambda), datos), interval = c(0.001, 100))$objective
incremento_lambda <- d_lambda_val / optimize(function(x) -log_verosimilitud_gamma(c(alpha, x), datos), interval = c(0.001, 100))$objective
alpha <- alpha - incremento_alpha
lambda <- lambda - incremento_lambda
}
return(c(alpha, lambda))
}
# Pruebo con 1000 datos
set.seed(456)
n <- 1000
shape <- 2
rate <- 0.5
datos_gamma <- rgamma(n, shape = shape, scale = 1/rate)
# Estimar parámetros por Newton-Raphson con 10 iteraciones
estimacion <- newton_raphson(datos_gamma, i = 10)
estimacion
error_estimacion <- c(abs(estimacion[1] - shape), abs(estimacion[2] - rate))
error_estimacion
library(MASS)
log_verosimilitud_gamma <- function(params, datos) {
alpha <- params[1]
lambda <- params[2]
n <- length(datos)
log_verosimilitud <- n * (alpha * log(lambda) - lgamma(alpha)) +
(alpha - 1) * sum(log(datos)) - lambda * sum(datos)
return(-log_verosimilitud)  # Se retorna el negativo para maximizar
}
derivada_parcial_alpha <- function(alpha, lambda, datos) {
n <- length(datos)
return(n * (digamma(alpha) - log(lambda)) + sum(log(datos)))
}
derivada_parcial_lambda <- function(alpha, lambda, datos) {
return(-sum(datos) + n * alpha / lambda)
}
newton_raphson <- function(datos, i) {
#Arranco con los de momentos como X_0
alpha_mom <- mean(datos)^2 / var(datos)
lambda_mom <- mean(datos) / var(datos)
alpha <- alpha_mom
lambda <- lambda_mom
for (iter in 1:i) {
# Calcular derivadas
d_alpha_val <- derivada_parcial_alpha(alpha, lambda, datos)
d_lambda_val <- derivada_parcial_lambda(alpha, lambda, datos)
# Calcular incremento de parámetros. Lo de optimize me lo sugirió chat-gpt para que queden mejor las divisiones
incremento_alpha <- d_alpha_val / optimize(function(x) -log_verosimilitud_gamma(c(x, lambda), datos), interval = c(0.001, 100))$objective
incremento_lambda <- d_lambda_val / optimize(function(x) -log_verosimilitud_gamma(c(alpha, x), datos), interval = c(0.001, 100))$objective
alpha <- alpha - incremento_alpha
lambda <- lambda - incremento_lambda
}
return(c(alpha, lambda))
}
'
# Pruebo con 1000 datos
set.seed(456)
n <- 1000
shape <- 2
rate <- 0.5
datos_gamma <- rgamma(n, shape = shape, scale = 1/rate)
# Estimar parámetros por Newton-Raphson con 10 iteraciones
estimacion <- newton_raphson(datos_gamma, i = 10)
estimacion
error_estimacion <- c(abs(estimacion[1] - shape), abs(estimacion[2] - rate))
error_estimacion
# Pruebo con 1000 datos
set.seed(456)
n <- 1000
shape <- 2
rate <- 0.5
datos_gamma <- rgamma(n, shape = shape, scale = 1/rate)
# Estimar parámetros por Newton-Raphson con 10 iteraciones
estimacion <- newton_raphson(datos_gamma, i = 10)
estimacion
error_estimacion <- c(abs(estimacion[1] - shape), abs(estimacion[2] - rate))
error_estimacion
# Pruebo con 1000 datos
set.seed(456)
n <- 1000
shape <- 3
rate <- 0.25
datos_gamma <- rgamma(n, shape = shape, scale = 1/rate)
# Estimar parámetros por Newton-Raphson con 10 iteraciones
estimacion <- newton_raphson(datos_gamma, i = 10)
estimacion
error_estimacion <- c(abs(estimacion[1] - shape), abs(estimacion[2] - rate))
error_estimacion
